**á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€ BPE á€á€…á€ºá€™á€»á€­á€¯á€¸á€á€Šá€ºá€¸á€•á€² á€á€¯á€¶á€¸á€á€¬ á€™á€Ÿá€¯á€á€ºá€•á€«á€˜á€°á€¸**á‹
LLM / NLP model á€á€½á€±á€™á€¾á€¬ **tokenizer algorithm á€™á€»á€­á€¯á€¸á€…á€¯á€¶** á€á€¯á€¶á€¸á€€á€¼á€•á€¼á€®á€¸
model design + language target á€•á€±á€«á€ºá€™á€°á€á€Šá€ºá€•á€«á€á€šá€ºá‹

---

## 1 Tokenizer Algorithm á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ á€¡á€€á€¼á€™á€ºá€¸á€–á€»á€‰á€ºá€¸

| Tokenizer  | Idea                       |
| ---------- | -------------------------- |
| BPE        | Frequent pair merge        |
| WordPiece  | Likelihood-based merge     |
| Unigram LM | Probabilistic segmentation |
| Character  | Char-level                 |
| Byte-level | Raw bytes                  |

---

## 2 Model Family á€¡á€œá€­á€¯á€€á€º á€˜á€¬á€á€¯á€¶á€¸á€œá€²?

---

### ğŸ”¹ GPT Series (GPT-2 / GPT-3 / GPT-4*)

**Tokenizer:** Byte-level BPE

* Byte (0â€“255) á€€á€”á€±á€…
* OOV á€™á€›á€¾á€­
* English-centric
* Space-aware (`Ä word`)

ğŸ‘‰ **GPT = BPE (byte-level)**

---

### ğŸ”¹ LLaMA / LLaMA-2 / LLaMA-3

**Tokenizer:** SentencePiece (BPE variant)

* Subword BPE
* No explicit space token
* Unicode-based
* Multilingual á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸

ğŸ‘‰ **LLaMA = BPE (SentencePiece)**

---

### ğŸ”¹ BERT / RoBERTa

**Tokenizer:** WordPiece

* Likelihood-based
* `[CLS]`, `[SEP]`
* Masked LM friendly

ğŸ‘‰ **BERT â‰  BPE (but similar idea)**

---

### ğŸ”¹ T5

**Tokenizer:** SentencePiece (Unigram LM)

* Probabilistic segmentation
* Multilingual friendly
* No whitespace dependency

ğŸ‘‰ **T5 = Unigram LM**

---

### ğŸ”¹ XLM-R

**Tokenizer:** SentencePiece (Unigram)

* 100+ languages
* Unicode normalization

---

### ğŸ”¹ BLOOM

**Tokenizer:** Byte-level BPE

* Similar to GPT
* Multilingual tuned

---

### ğŸ”¹ mBERT

**Tokenizer:** WordPiece

---

### ğŸ”¹ Whisper (Speech â†’ Text)

**Tokenizer:** Byte-level BPE

* Robust to noisy transcripts

---

## 3 BPE vs WordPiece vs Unigram (Conceptual)

| Feature      | BPE           | WordPiece           | Unigram       |
| ------------ | ------------- | ------------------- | ------------- |
| Training     | Greedy merge  | Likelihood maximize | LM-based      |
| Segmentation | Deterministic | Deterministic       | Probabilistic |
| Multilingual | Medium        | Medium              | High          |
| OOV          | âŒ             | âŒ                   | âŒ             |

---

## 4 á€˜á€¬á€€á€¼á€±á€¬á€„á€ºá€· Model á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸ á€™á€á€°á€á€¬á€œá€²?

### ğŸ”¹ Language property

* English â†’ BPE OK
* Burmese / Thai â†’ SentencePiece á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸

### ğŸ”¹ Training objective

* Causal LM â†’ BPE
* Masked LM â†’ WordPiece

### ğŸ”¹ Engineering

* Byte-level â†’ robust input
* Unicode-level â†’ compact vocab

---

## 5 Myanmar / Low-resource Language Perspective 

| Choice        | Result                       |
| ------------- | ---------------------------- |
| Byte BPE      | Robust but token length á€€á€¼á€®á€¸ |
| SentencePiece | Better segmentation          |
| WordPiece     | Needs whitespace             |

ğŸ‘‰ **Myanmar NLP â†’ SentencePiece (Unigram/BPE)** á€á€„á€·á€ºá€á€±á€¬á€º

---

### BPE á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º Sentence Piece á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º word pieces / á€•á€¼á€®á€¸á€á€½á€¬á€¸á€á€¬á€”á€²á€· embedding á€œá€¯á€•á€ºá€–á€­á€¯á€·á€œá€­á€¯á€¡á€•á€ºá€œá€¬á€•á€«á€•á€¼á€®á‹ Words embeddeding á€œá€¯á€•á€ºá€á€²á€·á€¡á€á€»á€­á€”á€ºá€™á€¾á€¬ á€á€¯á€¶á€¸á€œá€­á€¯á€·á€›á€á€²á€· algorithms á€á€½á€±á€€á€á€±á€¬á€· 
