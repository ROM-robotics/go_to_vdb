# ğŸ“š lora-tune.ipynb - á€¡á€á€±á€¸á€…á€­á€á€º á€á€„á€ºá€€á€¼á€¬á€¸á€á€»á€€á€º (á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬)

## ğŸ¯ á€’á€® Notebook á€€ á€˜á€¬á€œá€¯á€•á€ºá€á€¬á€œá€²?

á€’á€® notebook á€€ **LoRA (Low-Rank Adaptation)** á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€¼á€®á€¸ ROS 2 command generation model á€€á€­á€¯ fine-tune á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ LoRA á€€ á€á€­á€•á€ºá€€á€±á€¬á€„á€ºá€¸á€á€²á€· parameter-efficient fine-tuning á€”á€Šá€ºá€¸á€•á€Šá€¬á€–á€¼á€…á€ºá€•á€¼á€®á€¸ Hugging Face PEFT library á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹"Input á€€á€­á€¯á€•á€±á€¸á€•á€¼á€®á€¸ Output á€¡á€™á€¾á€”á€ºá€‘á€½á€€á€ºá€œá€¬á€¡á€±á€¬á€„á€º LoRA á€›á€²á€· layer á€¡á€á€…á€ºá€œá€±á€¸á€á€½á€±á€€á€­á€¯á€•á€² train á€á€¬á€•á€«" á€œá€­á€¯á€· á€•á€¼á€±á€¬á€œá€­á€¯á€·á€›á€•á€«á€á€šá€ºá‹

---

## ğŸ“– Section 01: Dataset Preparation (Dataset á€•á€¼á€„á€ºá€†á€„á€ºá€á€¼á€„á€ºá€¸)

```python
from datasets import Dataset
from transformers import (AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling)
from peft import LoraConfig, get_peft_model, TaskType
```

### á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

**Import á€™á€»á€¬á€¸**:
- **datasets**: Hugging Face Dataset library
- **transformers**: Model, Tokenizer, Trainer
- **peft**: LoRA á€¡á€á€½á€€á€º PEFT (Parameter-Efficient Fine-Tuning) library

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: PEFT library á€€ Meta/Hugging Face á€€ á€–á€”á€ºá€á€®á€¸á€‘á€¬á€¸á€á€²á€· professional tool á€•á€«á‹ LoRA, Prefix Tuning, P-Tuning á€…á€á€¬á€á€½á€± support á€œá€¯á€•á€ºá€á€šá€ºá‹

---

## ğŸ“Š Section 02: Dataset Creation

```python
data = {
    "instruction": [
        "Move robot forward at 0.5 m/s",
        "Turn robot left 90 degrees",
        "Stop the robot",
        "Navigate to position x=2, y=3",
        "Rotate robot clockwise"
    ],
    "output": [
        "ros2 topic pub /cmd_vel geometry_msgs/msg/Twist '{linear: {x: 0.5}}'",
        "ros2 topic pub /cmd_vel geometry_msgs/msg/Twist '{angular: {z: 1.57}}'",
        "ros2 topic pub /cmd_vel geometry_msgs/msg/Twist '{}'",
        "ros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose '{pose: {pose: {position: {x: 2.0, y: 3.0}}}}'",
        "ros2 topic pub /cmd_vel geometry_msgs/msg/Twist '{angular: {z: -1.57}}'"
    ]
}

dataset = Dataset.from_dict(data)
dataset = dataset.train_test_split(test_size=0.2)
print(dataset)
```

### á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

**Dataset Structure**:
- **instruction**: Human-readable commands (á€œá€°á€á€¯á€¶á€¸ instructions)
- **output**: ROS 2 commands (technical format)

**Train-Test Split**:
```python
train_test_split(test_size=0.2)
```
- 80% â†’ Training data
- 20% â†’ Test/Validation data

**Output**:
```
DatasetDict({
    train: Dataset({features: ['instruction', 'output'], num_rows: 4}),
    test: Dataset({features: ['instruction', 'output'], num_rows: 1})
})
```

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: Production á€™á€¾á€¬ data á€™á€»á€¬á€¸á€™á€»á€¬á€¸ á€œá€­á€¯á€•á€«á€á€šá€ºá‹ á€’á€®á€™á€¾á€¬ 5 examples á€•á€² á€›á€¾á€­á€á€¬á€€ demonstration á€¡á€á€½á€€á€ºá€•á€«á‹ á€¡á€”á€Šá€ºá€¸á€†á€¯á€¶á€¸ 100-1000+ examples á€á€¯á€¶á€¸á€á€„á€·á€ºá€•á€«á€á€šá€ºá‹

---

## ğŸ”¤ Section 03: Tokenizer & Tokenization

```python
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")
tokenizer.pad_token = tokenizer.eos_token
```

### á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

**Pad Token Setting**:
```python
tokenizer.pad_token = tokenizer.eos_token
```
- Qwen model á€™á€¾á€¬ pad_token á€™á€›á€¾á€­á€˜á€°á€¸
- EOS (End-of-Sequence) token á€€á€­á€¯ pad token á€¡á€–á€¼á€…á€º á€á€¯á€¶á€¸á€á€šá€º
- á€á€€á€šá€ºá€œá€­á€¯á€· eos_token á€á€±á€á€»á€¬á€™á€•á€«á€›á€„á€º Model á€€ á€™á€±á€¸á€á€½á€”á€ºá€¸á€–á€¼á€±á€•á€¼á€®á€¸á€á€¬á€á€±á€¬á€„á€º á€™á€›á€•á€ºá€˜á€² "á€€á€»á€±á€¸á€‡á€°á€¸á€á€„á€ºá€•á€«á€á€šá€ºáŠ á€”á€±á€¬á€€á€ºá€‘á€•á€º á€˜á€¬á€™á€±á€¸á€¦á€¸á€™á€œá€²áŠ á€”á€±á€€á€±á€¬á€„á€ºá€¸á€œá€¬á€¸..." á€…á€á€–á€¼á€„á€·á€º á€¡á€•á€­á€¯á€á€½á€± á€†á€€á€ºá€á€­á€¯á€€á€ºá€œá€»á€¾á€±á€¬á€€á€ºá€•á€¼á€±á€¬á€”á€±á€•á€«á€œá€­á€™á€·á€ºá€™á€šá€ºá‹
- á€’á€«á€€ batch training á€¡á€á€½á€€á€º á€œá€­á€¯á€¡á€•á€ºá€á€šá€º (sequences á€á€½á€±á€€á€­á€¯ á€á€°á€Šá€®á€á€²á€· length á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º)

**Tokenize Function**:
```python
def tokenize_function(examples):
    texts = [
        f"### Instruction:\n{inst}\n\n### Command:\n{out}"
        for inst, out in zip(examples["instruction"], examples["output"])
    ]
    
    tokenized = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=256,
    )
    
    labels = []
    for ids in tokenized["input_ids"]:
        labels.append([
            token if token != tokenizer.pad_token_id else -100
            for token in ids
        ])
    
    tokenized["labels"] = labels
    return tokenized
```

### á€¡á€†á€„á€·á€ºá€†á€„á€·á€º á€›á€¾á€„á€ºá€¸á€•á€¼á€á€»á€€á€º:

**Step 1: Format Text**
```python
texts = [
    f"### Instruction:\n{inst}\n\n### Command:\n{out}"
    for inst, out in zip(examples["instruction"], examples["output"])
]
```

Example output:
```
### Instruction:
Move robot forward at 0.5 m/s

### Command:
ros2 topic pub /cmd_vel geometry_msgs/msg/Twist '{linear: {x: 0.5}}'
```

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º á€’á€® format á€á€¯á€¶á€¸á€á€¬á€œá€²?**
- Model á€€ instruction-following format á€€á€­á€¯ á€•á€­á€¯á€”á€¬á€¸á€œá€Šá€ºá€œá€½á€šá€ºá€á€šá€º
- Clear separation between instruction and command
- Structured prompt engineering

**Step 2: Tokenization**
```python
tokenized = tokenizer(
    texts,
    padding="max_length",  # Pad to max_length
    truncation=True,       # Cut if longer than max_length
    max_length=256,        # Maximum sequence length
)
```

**Parameters explained**:
- **padding="max_length"**: á€¡á€¬á€¸á€œá€¯á€¶á€¸ 256 tokens á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º pad á€œá€¯á€•á€ºá€á€šá€º
- **truncation=True**: 256 á€‘á€€á€º á€•á€­á€¯á€›á€¾á€Šá€ºá€›á€„á€º á€–á€¼á€á€ºá€á€šá€º
- **max_length=256**: Maximum length á€á€á€ºá€™á€¾á€á€ºá€á€»á€€á€º

**Step 3: Create Labels**
```python
labels = []
for ids in tokenized["input_ids"]:
    labels.append([
        token if token != tokenizer.pad_token_id else -100
        for token in ids
    ])
```

**-100 á€€ á€˜á€¬á€œá€²?**
- PyTorch á€™á€¾á€¬ -100 á€€ "ignore this token in loss calculation" á€€á€­á€¯ á€†á€­á€¯á€œá€­á€¯á€á€šá€º
- Pad tokens á€á€½á€±á€€á€­á€¯ loss á€™á€á€½á€€á€ºá€˜á€°á€¸ (á€¡á€“á€­á€•á€¹á€•á€«á€šá€º á€™á€›á€¾á€­á€œá€­á€¯á€·)

**Apply to Dataset**:
```python
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)
```

- **batched=True**: Batch á€œá€­á€¯á€€á€º process á€œá€¯á€•á€ºá€á€šá€º (á€™á€¼á€”á€ºá€á€šá€º)
- **remove_columns**: Original columns (instruction, output) á€€á€­á€¯ á€–á€»á€€á€ºá€á€šá€ºáŠ tokenized versions á€•á€² á€á€­á€™á€ºá€¸á€á€šá€º

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: Tokenization á€€ NLP pipeline á€™á€¾á€¬ á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ á€¡á€†á€„á€·á€ºá€á€…á€ºá€á€¯á€•á€«á‹ Format á€™á€¾á€”á€ºá€›á€„á€º model á€€ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€¡á€±á€¬á€„á€º á€á€„á€ºá€šá€°á€”á€­á€¯á€„á€ºá€á€šá€ºá‹

---

## ğŸ”§ Section 04: LoRA Configuration

```python
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
)
```

### á€¡á€á€±á€¸á€…á€­á€á€º á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

### 1ï¸âƒ£ Task Type
```python
task_type=TaskType.CAUSAL_LM
```
- **CAUSAL_LM**: Causal Language Modeling (next token prediction)
- Text generation tasks á€¡á€á€½á€€á€º
- GPT-style models á€™á€»á€¬á€¸á€¡á€á€½á€€á€º

### 2ï¸âƒ£ Rank (r)
```python
r=8
```

**LoRA á€€ á€˜á€šá€ºá€œá€­á€¯ á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€á€œá€²?**

Original weight matrix: `W` (size: `d Ã— k`)

LoRA á€€ á€’á€® matrix á€€á€­á€¯ **update á€™á€œá€¯á€•á€ºá€˜á€°á€¸**á‹ á€¡á€…á€¬á€¸:
```
W' = W + A Ã— B
```
- `A`: Matrix size `d Ã— r`
- `B`: Matrix size `r Ã— k`
- `r`: **Rank** (low-rank)

**Example**:
- Original: `W` = 4096 Ã— 4096 = 16,777,216 parameters
- LoRA (r=8): 
  - `A` = 4096 Ã— 8 = 32,768
  - `B` = 8 Ã— 4096 = 32,768
  - **Total** = 65,536 parameters (0.4% of original!)

**r á€›á€²á€· á€¡á€€á€»á€­á€¯á€¸á€á€€á€ºá€›á€±á€¬á€€á€ºá€™á€¾á€¯**:
- **r á€á€±á€¸á€œá€±**: Parameters á€”á€Šá€ºá€¸á€œá€±áŠ memory á€á€€á€ºá€á€¬á€œá€±áŠ á€•á€±á€™á€šá€·á€º expressiveness á€”á€Šá€ºá€¸á€á€šá€º
- **r á€€á€¼á€®á€¸á€œá€±**: Parameters á€™á€»á€¬á€¸á€œá€±áŠ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€²á€· performance á€›á€”á€­á€¯á€„á€ºá€á€šá€º

**á€•á€¯á€¶á€™á€¾á€”á€º values**: r = 4, 8, 16, 32

### 3ï¸âƒ£ LoRA Alpha
```python
lora_alpha=32
```

**Alpha á€€ á€˜á€¬á€œá€²?**
- Scaling factor á€•á€«
- LoRA updates á€€á€­á€¯ scale á€œá€¯á€•á€ºá€–á€­á€¯á€·
- lora_alpha=32 á€†á€­á€¯á€á€¬ "á€„á€«á€á€„á€ºá€•á€±á€¸á€á€²á€· á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€¡á€á€…á€ºá€€á€­á€¯ (á„) á€†á€œá€±á€¬á€€á€º á€¡á€œá€±á€¸á€‘á€¬á€¸á€•á€¼á€®á€¸ á€¡á€á€¯á€¶á€¸á€á€»á€•á€«" á€œá€­á€¯á€· Model á€€á€­á€¯ á€¡á€™á€­á€”á€·á€ºá€•á€±á€¸á€œá€­á€¯á€€á€ºá€á€¬á€•á€«á‹

**Formula**:
```
scaling = lora_alpha / r
```

Example (r=8, alpha=32):
```
scaling = 32 / 8 = 4
```

**Update**:
```
W' = W + (alpha/r) Ã— A Ã— B
```

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º á€œá€­á€¯á€á€¬á€œá€²?**
- r á€•á€¼á€±á€¬á€„á€ºá€¸á€›á€„á€ºá€œá€Šá€ºá€¸ learning rate á€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€»á€¯á€•á€ºá€”á€­á€¯á€„á€ºá€–á€­á€¯á€·
- Hyperparameter tuning á€€á€­á€¯ á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€…á€±á€–á€­á€¯á€·

**Best practice**: `alpha = 2 Ã— r` or `alpha = 4 Ã— r`

### 4ï¸âƒ£ LoRA Dropout
```python
lora_dropout=0.1
```

- LoRA layers á€™á€¾á€¬ 10% dropout á€á€¯á€¶á€¸á€á€šá€º
- Overfitting á€€á€­á€¯ á€€á€¬á€€á€½á€šá€ºá€–á€­á€¯á€·
- Regularization technique

### 5ï¸âƒ£ Target Modules
```python
target_modules=[
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
```

**Transformer Architecture á€™á€¾á€¬ á€˜á€šá€º layers á€á€½á€±á€œá€²?**

**Attention Layers**:
- **q_proj**: Query projection (What am I looking for?)
- **k_proj**: Key projection (What do I have?)
- **v_proj**: Value projection (What information do I get?)
- **o_proj**: Output projection (Combine attention results)

**MLP (Feed-Forward) Layers**:
- **gate_proj**: Gate projection (SwiGLU activation)
- **up_proj**: Up projection (Expand dimension)
- **down_proj**: Down projection (Reduce dimension)

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º á€¡á€¬á€¸á€œá€¯á€¶á€¸ select á€œá€¯á€•á€ºá€‘á€¬á€¸á€á€¬á€œá€²?**
- á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€²á€· performance á€¡á€á€½á€€á€º
- Model capacity á€•á€­á€¯á€™á€»á€¬á€¸á€–á€­á€¯á€·

**Alternative**:
```python
# Only attention (memory á€á€€á€ºá€á€¬á€á€šá€º)
target_modules=["q_proj", "v_proj"]
```

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: LoRA configuration á€€ performance á€”á€²á€· efficiency á€€á€¼á€¬á€¸ balance á€á€»á€­á€”á€ºá€Šá€¾á€­á€á€¬á€•á€«á‹ r á€”á€²á€· alpha á€€ á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ parameters á€á€½á€±á€•á€«á‹

---

## ğŸ¤– Section 05: Base Model â†’ PEFT Model

```python
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-1.5B-Instruct",
    device_map="auto"
)

peft_model = get_peft_model(base_model, lora_config)
peft_model.print_trainable_parameters()
```

### á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

**Step 1: Load Base Model**
```python
base_model = AutoModelForCausalLM.from_pretrained(...)
```
- 1.5 billion parameters á€›á€¾á€­á€á€²á€· model load á€œá€¯á€•á€ºá€á€šá€º
- **device_map="auto"**: GPU/CPU á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€…á€®á€™á€¶á€•á€±á€¸á€á€šá€º

**Step 2: Apply LoRA**
```python
peft_model = get_peft_model(base_model, lora_config)
```

**á€’á€®á€™á€¾á€¬ á€˜á€¬á€–á€¼á€…á€ºá€á€¬á€œá€²?**
1. Base model á€›á€²á€· á€¡á€¬á€¸á€œá€¯á€¶á€¸ parameters á€€á€­á€¯ **freeze** á€œá€¯á€•á€ºá€á€šá€º
2. Target modules á€á€½á€±á€™á€¾á€¬ LoRA adapters (A, B matrices) á€‘á€Šá€·á€ºá€á€šá€º
3. LoRA parameters á€á€½á€±á€€á€­á€¯á€•á€² **trainable** á€¡á€–á€¼á€…á€º á€á€á€ºá€™á€¾á€á€ºá€á€šá€º

**Step 3: Print Trainable Parameters**
```python
peft_model.print_trainable_parameters()
```

**Output example**:
```
trainable params: 2,359,296 || all params: 1,547,359,296 || trainable%: 0.15%
```

**Analysis**:
- **Total parameters**: 1.5 billion
- **Trainable parameters**: 2.3 million (0.15%)
- **99.85% frozen!** ğŸ”’

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: LoRA á€€ model á€›á€²á€· 0.15% á€€á€­á€¯á€•á€² train á€œá€¯á€•á€ºá€á€šá€ºá‹ á€’á€«á€•á€±á€™á€šá€·á€º performance á€€ full fine-tuning á€”á€²á€· á€”á€®á€¸á€•á€«á€¸á€›á€•á€«á€á€šá€º! Magic! âœ¨

---

## ğŸ‹ï¸ Section 06: Training Setup

```python
training_args = TrainingArguments(
    output_dir="./ros2_lora_model",
    num_train_epochs=10,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_strategy="steps",
    logging_steps=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    fp16=True,
    report_to="none",
)
```

### Parameter á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

### 1ï¸âƒ£ Output Directory
```python
output_dir="./ros2_lora_model"
```
- Checkpoints á€á€­á€™á€ºá€¸á€™á€šá€·á€º folder
- Training á€•á€¼á€®á€¸á€›á€„á€º LoRA weights á€›á€™á€šá€º

### 2ï¸âƒ£ Training Duration
```python
num_train_epochs=10
```
- Dataset á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ 10 á€€á€¼á€­á€™á€º iterate á€œá€¯á€•á€ºá€™á€šá€º

### 3ï¸âƒ£ Batch Size
```python
per_device_train_batch_size=2
```
- GPU á€á€…á€ºá€á€¯á€™á€¾á€¬ á€á€…á€ºá€á€«á€á€…á€ºá€›á€¶ 2 examples train á€œá€¯á€•á€ºá€™á€šá€º
- Memory á€¡á€€á€”á€·á€ºá€¡á€á€á€º á€›á€¾á€­á€œá€­á€¯á€· á€á€±á€¸á€á€±á€¸ á€á€¯á€¶á€¸á€á€šá€º

### 4ï¸âƒ£ Gradient Accumulation
```python
gradient_accumulation_steps=4
```

**Important concept!**

- **Physical batch size**: 2
- **Effective batch size**: 2 Ã— 4 = **8**

**á€˜á€šá€ºá€œá€­á€¯ á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€á€œá€²?**
1. Forward pass 2 examples
2. Backward pass (gradients á€á€½á€€á€ºá€•á€±á€™á€šá€·á€º **á€™á€•á€¼á€±á€¬á€„á€ºá€¸á€á€±á€¸á€˜á€°á€¸**)
3. Repeat 4 times
4. á€•á€¼á€®á€¸á€™á€¾ parameters update á€œá€¯á€•á€ºá€á€šá€º

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º á€á€¯á€¶á€¸á€á€¬á€œá€²?**
- Batch size á€€á€¼á€®á€¸á€€á€¼á€®á€¸ á€á€¯á€¶á€¸á€á€œá€­á€¯ á€–á€¼á€…á€ºá€á€šá€º
- GPU memory á€™á€™á€»á€¬á€¸á€˜á€°á€¸
- Training stability á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€šá€º

### 5ï¸âƒ£ Learning Rate
```python
learning_rate=2e-4
```
- 0.0002
- LoRA á€¡á€á€½á€€á€º á€á€„á€·á€ºá€á€±á€¬á€ºá€á€²á€· learning rate
- Full fine-tuning (1e-5) á€‘á€€á€º á€•á€­á€¯á€€á€¼á€®á€¸á€á€šá€º

### 6ï¸âƒ£ Logging
```python
logging_strategy="steps"
logging_steps=1
```
- Every step á€™á€¾á€¬ loss print á€œá€¯á€•á€ºá€™á€šá€º
- Progress monitor á€œá€¯á€•á€ºá€–á€­á€¯á€·

### 7ï¸âƒ£ Evaluation & Saving
```python
eval_strategy="epoch"
save_strategy="epoch"
```
- Every epoch á€•á€¼á€®á€¸á€›á€„á€º evaluation run á€™á€šá€º
- Every epoch á€•á€¼á€®á€¸á€›á€„á€º checkpoint á€á€­á€™á€ºá€¸á€™á€šá€º

### 8ï¸âƒ£ Mixed Precision
```python
fp16=True
```
- **Float16 (16-bit)** á€á€¯á€¶á€¸á€™á€šá€º
- GPU memory 50% á€á€€á€ºá€á€¬á€á€šá€º
- Training 2-3x á€™á€¼á€”á€ºá€á€šá€º
- Modern GPUs á€™á€¾á€¬ recommended

### 9ï¸âƒ£ Reporting
```python
report_to="none"
```
- WandB, TensorBoard á€…á€á€¬á€á€½á€±á€€á€­á€¯ á€™á€á€¯á€¶á€¸á€˜á€°á€¸
- Simple training á€¡á€á€½á€€á€º

### Data Collator
```python
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)
```

- **mlm=False**: Masked Language Modeling á€™á€Ÿá€¯á€á€ºá€˜á€°á€¸
- Causal LM (next token prediction) á€á€¯á€¶á€¸á€™á€šá€º

### Trainer Setup
```python
trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
)
```

- Hugging Face Trainer API á€á€¯á€¶á€¸á€á€šá€º
- Training loop á€€á€­á€¯ automatic á€…á€®á€™á€¶á€•á€±á€¸á€á€šá€º

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: Trainer API á€€ training á€€á€­á€¯ á€¡á€›á€™á€ºá€¸ á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€…á€±á€á€šá€ºá‹ Manual loop á€™á€›á€±á€¸á€›á€•á€±á€™á€šá€·á€º flexible configuration á€›á€¾á€­á€•á€«á€á€šá€ºá‹

---

## ğŸš€ Training & Saving

```python
print("Starting training...")
trainer.train()

trainer.save_model("./ros2_command_model_final")
print("Training complete!")
```

### á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

**trainer.train()**:
- Training loop á€…á€á€„á€ºá€á€šá€º
- Automatic:
  - Forward/backward passes
  - Gradient updates
  - Logging
  - Evaluation
  - Checkpointing

**trainer.save_model()**:
- Final LoRA weights á€á€­á€™á€ºá€¸á€á€šá€º
- Adapter config á€á€­á€™á€ºá€¸á€á€šá€º

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: Simple API call á€á€…á€ºá€á€¯á€”á€²á€· professional training pipeline á€¡á€€á€¯á€”á€º á€›á€•á€«á€á€šá€º!

---

## ğŸ“¦ Section 07: Export Models (Zip)

```python
import shutil

shutil.make_archive("/kaggle/working/ros2_command_model_final", 'zip', "/kaggle/working/ros2_command_model_final")
shutil.make_archive("/kaggle/working/ros2_lora_model", 'zip', "/kaggle/working/ros2_lora_model")
```

### á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:
- Folders á€á€½á€±á€€á€­á€¯ zip archives á€¡á€–á€¼á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€á€šá€º
- Download á€œá€¯á€•á€ºá€–á€­á€¯á€· á€•á€­á€¯á€œá€½á€šá€ºá€á€šá€º (Kaggle environment)

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: Cloud notebooks (Kaggle, Colab) á€™á€¾á€¬ results export á€œá€¯á€•á€ºá€–á€­á€¯á€· á€™á€™á€±á€·á€•á€«á€”á€²á€·á‹

---

## ğŸ§ª Section 08: LoRA Testing (Inference)

```python
from peft import PeftModel

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-1.5B-Instruct",
    device_map="auto"
)

peft_model = PeftModel.from_pretrained(
    base_model,
    "./ros2_command_model_final"
)

peft_model.eval()
```

### á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:

**Loading Process**:
1. **Tokenizer load**: á€¡á€›á€„á€ºá€”á€²á€· á€¡á€á€°á€á€°
2. **Base model load**: Original pretrained model
3. **LoRA load**: Trained adapters á€€á€­á€¯ attach á€œá€¯á€•á€ºá€á€šá€º
4. **Eval mode**: Inference á€¡á€á€½á€€á€º

**PeftModel.from_pretrained()**:
- Base model + LoRA adapters á€€á€­á€¯ merge á€œá€¯á€•á€ºá€á€šá€º
- Ready for inference

### Inference Example

```python
prompt = """### Instruction:
Move robot forward 3 meters

### Command:
"""

inputs = tokenizer(prompt, return_tensors="pt").to(peft_model.device)

outputs = peft_model.generate(
    **inputs,
    max_new_tokens=50,
    temperature=0.7,
    do_sample=True,
    eos_token_id=tokenizer.eos_token_id,
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### Generation Parameters:

**max_new_tokens=50**:
- á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ 50 tokens generate á€™á€šá€º

**temperature=0.7**:
- Randomness control
- 0.0 = deterministic (á€¡á€™á€¼á€²á€á€™á€ºá€¸ á€á€°á€á€²á€· output)
- 1.0 = maximum randomness
- 0.7 = balanced (creativity + consistency)

**do_sample=True**:
- Sampling á€á€¯á€¶á€¸á€™á€šá€º (random selection)
- False á€†á€­á€¯á€›á€„á€º greedy (á€¡á€™á€¼á€²á€á€™á€ºá€¸ highest probability token)

**eos_token_id**:
- Generation á€˜á€šá€ºá€¡á€á€»á€­á€”á€º á€›á€•á€ºá€›á€™á€œá€² á€á€á€ºá€™á€¾á€á€ºá€á€šá€º

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬**: Inference á€€ training á€‘á€€á€º á€•á€­á€¯á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€•á€±á€™á€šá€·á€º generation parameters á€€ output quality á€€á€­á€¯ á€á€€á€ºá€›á€±á€¬á€€á€ºá€á€šá€ºá‹

---

## ğŸ” LoRA vs Soft Prompt vs P-Tuning á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€á€»á€€á€º

| Feature | Soft Prompt | P-Tuning v2 | LoRA |
|---------|-------------|-------------|------|
| **Trainable Params** | ~20K | ~100K | ~2M |
| **Base Model** | Frozen â„ï¸ | Frozen â„ï¸ | Frozen â„ï¸ |
| **Training Method** | Prompt embeddings | Prompt embeddings + MLP | Low-rank adapters |
| **Training Speed** | âš¡ á€¡á€™á€¼á€”á€ºá€†á€¯á€¶á€¸ | âš¡ á€™á€¼á€”á€ºá€á€šá€º | ğŸ¢ á€¡á€”á€Šá€ºá€¸á€„á€šá€º á€”á€¾á€±á€¸á€á€šá€º |
| **Memory Usage** | ğŸ’š á€¡á€”á€Šá€ºá€¸á€†á€¯á€¶á€¸ | ğŸ’š á€”á€Šá€ºá€¸á€á€šá€º | ğŸ’› Moderate |
| **Performance** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **Flexibility** | Low | Medium | High |
| **Best For** | Simple tasks | Medium tasks | Complex tasks |
| **Industry Usage** | Research | Research | Production âœ… |
| **Deployment** | Easy | Easy | Easy |
| **Multi-task** | âœ… Easy | âœ… Easy | âœ… Very easy |

---

## ğŸ“ á€¡á€“á€­á€€ á€á€„á€ºá€á€”á€ºá€¸á€…á€¬á€™á€»á€¬á€¸

### âœ… LoRA á€›á€²á€· á€¡á€¬á€¸á€á€¬á€á€»á€€á€ºá€™á€»á€¬á€¸:

1. **Parameter Efficiency**: 0.15% parameters á€•á€² train á€œá€¯á€•á€ºá€á€šá€º
2. **Performance**: Full fine-tuning á€”á€²á€· á€”á€®á€¸á€•á€«á€¸
3. **Memory Efficient**: GPU memory á€á€€á€ºá€á€¬á€á€šá€º
4. **Fast Training**: Small parameters á€œá€­á€¯á€· á€™á€¼á€”á€ºá€á€šá€º
5. **Easy Deployment**: Adapter swap á€œá€¯á€•á€ºá€œá€­á€¯á€·á€›á€á€šá€º
6. **Multi-task**: Task á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€¡á€á€½á€€á€º adapter á€á€­á€™á€ºá€¸á€œá€­á€¯á€·á€›á€á€šá€º

### ğŸ“Œ LoRA á€˜á€šá€ºá€œá€­á€¯ á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€á€œá€²?

**Mathematical Foundation**:
```
W' = W + Î”W
Î”W = A Ã— B
```

Where:
- `W`: Original weight matrix (frozen)
- `A`: Matrix `d Ã— r`
- `B`: Matrix `r Ã— k`
- `r`: Rank (r << d, k)

**Example**:
```
Original: 4096 Ã— 4096 = 16M params
LoRA (r=8): (4096Ã—8) + (8Ã—4096) = 65K params
Reduction: 99.6%! ğŸ‰
```

### ğŸ’¡ Best Practices:

**1. Rank Selection**:
- Simple tasks: r = 4-8
- Medium tasks: r = 16-32
- Complex tasks: r = 64-128

**2. Alpha Setting**:
- Rule: `alpha = 2r` or `alpha = 4r`
- Example: r=8 â†’ alpha=16 or 32

**3. Target Modules**:
- Minimal: `["q_proj", "v_proj"]`
- Recommended: All attention + MLP
- Trade-off: Coverage vs memory

**4. Learning Rate**:
- LoRA: 1e-4 to 3e-4
- Higher than full fine-tuning
- Monitor training loss

**5. Data Preparation**:
- Format consistency important
- Clear instruction-output separation
- Quality > Quantity (but both better!)

### ğŸ”§ Troubleshooting:

**Problem**: Training loss á€™á€€á€»á€˜á€°á€¸
- âœ… Learning rate á€œá€»á€¾á€±á€¬á€·á€€á€¼á€Šá€·á€ºá€•á€«
- âœ… Rank á€€á€­á€¯ á€á€­á€¯á€¸á€€á€¼á€Šá€·á€ºá€•á€«
- âœ… Data quality á€…á€…á€ºá€•á€«

**Problem**: GPU memory error
- âœ… Batch size á€œá€»á€¾á€±á€¬á€·á€•á€«
- âœ… fp16/bf16 á€á€¯á€¶á€¸á€•á€«
- âœ… Gradient checkpointing enable á€œá€¯á€•á€ºá€•á€«

**Problem**: Inference slow
- âœ… Merge LoRA weights (`merge_and_unload()`)
- âœ… Quantization (4-bit/8-bit)
- âœ… ONNX export

**Problem**: Overfitting
- âœ… Dropout á€á€­á€¯á€¸á€•á€«
- âœ… Early stopping á€á€¯á€¶á€¸á€•á€«
- âœ… Data augmentation á€œá€¯á€•á€ºá€•á€«

### ğŸš€ Advanced Tips:

**1. Merge Adapters**:
```python
merged_model = peft_model.merge_and_unload()
```
- LoRA á€€á€­á€¯ base model á€”á€²á€· merge á€œá€¯á€•á€ºá€á€šá€º
- Inference á€•á€­á€¯á€™á€¼á€”á€ºá€á€šá€º

**2. Multiple Adapters**:
```python
# Task 1 adapter
model.load_adapter("task1_adapter")

# Switch to Task 2
model.set_adapter("task2_adapter")
```
- Multi-task learning á€¡á€á€½á€€á€º

**3. Quantization**:
```python
from peft import prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)
```
- 4-bit/8-bit training
- Memory á€¡á€›á€™á€ºá€¸ á€á€€á€ºá€á€¬á€á€šá€º

---

## ğŸ¤” á€…á€‰á€ºá€¸á€…á€¬á€¸á€…á€›á€¬ á€™á€±á€¸á€á€½á€”á€ºá€¸á€™á€»á€¬á€¸:

1. **Rank á€€á€­á€¯ á€˜á€šá€ºá€œá€­á€¯ á€›á€½á€±á€¸á€›á€™á€œá€²?**
   - Dataset size á€”á€²á€· task complexity á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€•á€«
   - á€…á€™á€ºá€¸á€á€•á€ºá€€á€¼á€Šá€·á€ºá€•á€«: r=8, 16, 32

2. **LoRA vs Full Fine-tuning á€˜á€šá€ºá€¡á€á€»á€­á€”á€º á€á€¯á€¶á€¸á€›á€™á€œá€²?**
   - LoRA: Limited resources, fast iteration
   - Full: Maximum performance, unlimited resources

3. **Production á€™á€¾á€¬ deploy á€œá€¯á€•á€ºá€™á€šá€ºá€†á€­á€¯á€›á€„á€º?**
   - Merge adapters
   - Quantize model
   - Optimize inference (ONNX, TensorRT)

4. **Multi-task learning á€œá€¯á€•á€ºá€á€»á€„á€ºá€›á€„á€º?**
   - Task á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€¡á€á€½á€€á€º adapter á€á€®á€¸á€á€¼á€¬á€¸ train á€œá€¯á€•á€ºá€•á€«
   - Runtime á€™á€¾á€¬ swap á€œá€¯á€•á€ºá€•á€«

---

## ğŸ“š á€”á€±á€¬á€€á€ºá€‘á€•á€º á€á€„á€ºá€šá€°á€…á€›á€¬á€™á€»á€¬á€¸:

1. **QLoRA**: Quantized LoRA (4-bit training)
2. **AdaLoRA**: Adaptive rank allocation
3. **IAÂ³**: (Infused Adapter by Inhibiting and Amplifying Inner Activations)
4. **Multi-adapter fusion**: Multiple adapters combine á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

---

## ğŸ¯ á€”á€­á€‚á€¯á€¶á€¸:

LoRA á€€ parameter-efficient fine-tuning á€™á€¾á€¬ **industry standard** á€–á€¼á€…á€ºá€á€½á€¬á€¸á€•á€«á€•á€¼á€®á‹ Research á€”á€²á€· production á€”á€¾á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€™á€¾á€¬ á€¡á€á€¯á€¶á€¸á€™á€»á€¬á€¸á€•á€«á€á€šá€ºá‹ 

**Key Takeaways**:
- âœ… 0.15% parameters á€•á€² train á€œá€¯á€•á€ºá€•á€±á€™á€šá€·á€º full fine-tuning á€”á€²á€· comparable
- âœ… Memory efficient, fast training
- âœ… Easy deployment, adapter swapping
- âœ… Production-ready á€”á€Šá€ºá€¸á€•á€Šá€¬

**á€…á€™á€ºá€¸á€á€•á€ºá€€á€¼á€Šá€·á€ºá€•á€«!** ğŸš€ LoRA á€€ AI democratization á€¡á€á€½á€€á€º á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€á€²á€· breakthrough á€•á€«á‹ Limited resources á€”á€²á€·á€á€±á€¬á€„á€º large models á€á€½á€±á€€á€­á€¯ fine-tune á€œá€¯á€•á€ºá€œá€­á€¯á€·á€›á€•á€«á€á€šá€º!
